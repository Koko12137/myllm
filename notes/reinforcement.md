# 3. Reinforcement Learning

## **3.1 NOTES:**

### [2025.3.4]

- 蒙特卡洛方法与蒙特卡洛树方法

    Bilibili: [AI如何下棋？直观了解蒙特卡洛树搜索MCTS！！！](https://www.bilibili.com/video/BV1JD4y1Q7mV)

### [2025.3.5]

- 为什么Qwen能自我改进推理，Llama却不行？斯坦福找到了原理
  
  原文：[知乎机械之心](https://zhuanlan.zhihu.com/p/28137897663)

  - 问题：在同样的强化学习训练下，不同模型自我改进的能力却存在很大差异。比如在一个游戏中，Qwen-2.5-3B 的自我改进能力远远超过 Llama-3.2-3B（两个模型初始都很差，但强化学习训练结束后，Qwen 达到约 60% 的准确率，Llama 只有 30%）。这是什么原因？

  - 做了什么：

      1. 开发了一个框架来分析对解决问题有用的认知行为，其中描述了四种关键的认知行为：验证（系统错误检查）、回溯（放弃失败的方法）、子目标设定（将问题分解为可管理的步骤）和逆向思考（从期望结果推理到初始输入）

        1、回溯或在检测到错误时显式修改方法（例如，「这种方法行不通，因为...」）；

        2、验证或系统地检查中间结果（例如，「让我们通过... 来验证这个结果」）；

        3、子目标设定，即将复杂问题分解为可管理的步骤（例如，「要解决这个问题，我们首先需要...」）；

        4、逆向思考，即在目标导向的推理问题中，从期望的结果出发，逐步向后推导，找到解决问题的路径。（例如，「要达到 75 的目标，我们需要一个能被... 整除的数字」）。

      2. 初步分析表明，Qwen 自然地表现出了这些推理行为，特别是验证和回溯，而 Llama 则缺乏这些行为。

  - 作者推论：

      1. AI 模型要想在有更多时间思考时真正变得更聪明，必须先具备一些基本的思考能力（比如检查错误、验证结果的习惯）。这些认知行为是 AI 模型自我改进的基础。

      2. 如果模型一开始就不会这些基本思考方法，即使给它再多的思考时间和计算资源，它也无法有效利用这些资源来提高自己的表现

  - 实验：

      1. 选择 Countdown 进行分析，因为它需要数学推理、规划和搜索策略等多种认知行为

      2. 策划了七个不同的启动数据集

        ① 其中五个数据集强调不同的行为组合：所有策略组合、仅回溯、回溯与验证、回溯与子目标设定以及回溯与逆向思考，使用 Claude-3.5-Sonnet 生成这些数据集

        ② 两个控制条件：一个空的思维链和一个与所有策略数据集的数据点长度匹配的填充占位符 token 的链，空的思维链验证正确与错误的区别，占位符研究有和没有的区别

      3. 测试人为增加认知行为的接触是否会增强自我提升的潜力

        ① 创建了两个对比集：一个具有认知行为，另一个极少认知内容的控制集

  - 结论：

      1. 通过用包含这些行为（尤其是回溯）的人工合成推理轨迹对 Llama 进行引导，可以使其在强化学习过程中表现大幅改善，甚至能达到与 Qwen 相当的性能提升

        ① 当用空的思维链控制进行启动时，在两种情况下，模型的性能都与基本 Llama 模型相当（≈30-35%），这表明仅仅分配额外的 token 而不包含认知行为无法有效利用测试时间计算

        ② 用不正确的解决方案启动但具有正确行为的模型，与在具有正确解决方案的数据集上训练的模型具有相同的性能，表明**认知行为的存在比结果的正确性更重要**

        ③ 对于LLaMA原模型与变体，行为丰富模型实现了与 Qwen 相当的性能，而控制模型的改进有限，行为丰富变体在整个训练过程中保持推理行为的高激活度，而控制模型表现出与基本 Llama 模型类似的行为

      2. RL 会选择性地放大经验上有用的行为，同时抑制其他行为

        ① 在全策略条件下，模型保留并加强回溯和验证，同时减少逆向思考和子目标设定。然而，当仅与回溯配对时，被抑制的行为（逆向思考和子目标设定）会在整个训练过程中持续存在。
